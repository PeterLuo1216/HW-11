{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f3f1f4",
   "metadata": {},
   "source": [
    "2: Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following metrics below: provide your answers and, in your own words, concisely explain your rationale for your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b9214",
   "metadata": {},
   "source": [
    "(1): Accuracy is great for balanced datasets where false positives and false negatives have similar costs, like predicting weather. (2): Sensitivity (recall) is crucial in situations like medical diagnoses, where missing positive cases, like a disease, could be dangerous. (3)Specificity is useful when it’s important to correctly identify negatives, like avoiding false positives in drug tests to protect someone’s reputation. (4): Precision works best in cases like spam detection, where it’s important that flagged positives (spam emails) are actually correct, avoiding unnecessary misclassification of important emails. Each metric has its place, depending on the trade-offs we care about most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fd8fd",
   "metadata": {},
   "source": [
    "4: Create an 80/20 split with 80% of the data as a training set ab_reduced_noNaN_train and 20% of the data testing set ab_reduced_noNaN_test using either df.sample(...) as done in TUT or using train_test_split(...) as done in the previous HW, and report on how many observations there are in the training data set and the test data set.\n",
    "\n",
    "Tell a ChatBot that you are about to fit a \"scikit-learn\" DecisionTreeClassifier model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree clf using only the List Price variable to predict whether or not a book is a hard cover or paper back book using a max_depth of 2; finally use tree.plot_tree(clf) to explain what predictions are made based on List Price for the fitted clf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac49fde",
   "metadata": {},
   "source": [
    "We will split the data into training and testing sets with 80% for training and 20% for testing using train_test_split. Setting a random seed ensures reproducibility. Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e89502",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m ab_reduced_noNaN_train, ab_reduced_noNaN_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mab_reduced_noNaN\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ab_reduced_noNaN_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ab_reduced_noNaN_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(ab_reduced_noNaN_train)}\")\n",
    "print(f\"Test set size: {len(ab_reduced_noNaN_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f02d94",
   "metadata": {},
   "source": [
    "The .fit() method trains the decision tree on the given features (X) and target variable (y). Here's a breakdown of the two steps:\n",
    "\n",
    "y = pd.get_dummies(...):\n",
    "Converts the categorical variable Hard_or_Paper into dummy variables, creating a binary column H where 1 represents hardcover and 0 represents paperback.\n",
    "\n",
    "X = ab_reduced_noNaN[['List Price']]:\n",
    "Selects List Price as the only feature to predict whether a book is hardcover or paperback.\n",
    "Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9802bdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(\u001b[43mab_reduced_noNaN\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m ab_reduced_noNaN[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import plotly.io as pio\n",
    "\n",
    "# Step 1: Initialize the classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Step 2: Fit the classifier\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Step 3: Visualize the decision tree using Plotly\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(clf, feature_names=[\"List Price\"], class_names=[\"Paperback\", \"Hardcover\"], filled=True)\n",
    "fig.show(renderer=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523aecd",
   "metadata": {},
   "source": [
    "The decision tree visualization will show how the List Price feature splits the data into different groups. For example:\n",
    "\n",
    "The first node might split based on whether the list price is less than $20.\n",
    "A second node could split further for prices less than $10 or between $10 and $20.\n",
    "Each leaf node will predict whether the book is likely to be hardcover or paperback based on the majority class in that group. The tree’s depth of 2 ensures simplicity, resulting in no more than two splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce719e",
   "metadata": {},
   "source": [
    "6: Use previously created ab_reduced_noNaN_test to create confusion matrices for clf and clf2. Report the sensitivity, specificity and accuracy for each of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9516a",
   "metadata": {},
   "source": [
    "1: Load Test Data\n",
    "Ensure that ab_reduced_noNaN_test is available and structured similarly to ab_reduced_noNaN (used for training). It should have columns \"List Price\" and \"Hard_or_Paper\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc61e0",
   "metadata": {},
   "source": [
    "2: Generate Predictions\n",
    "Use the test dataset to make predictions with both clf and clf2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c550de5",
   "metadata": {},
   "source": [
    "3: Compute Metrics\n",
    "Sensitivity (Recall)\n",
    "Specificity\n",
    "Accuracy\n",
    "code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078376f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "X_test = ab_reduced_noNaN_test[['List Price']]\n",
    "\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "y_pred_clf2 = clf2.predict(X_test)\n",
    "\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "def compute_metrics(cm):\n",
    "    TP = cm[1, 1]\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return sensitivity, specificity, accuracy\n",
    "\n",
    "sensitivity_clf, specificity_clf, accuracy_clf = compute_metrics(cm_clf)\n",
    "\n",
    "sensitivity_clf2, specificity_clf2, accuracy_clf2 = compute_metrics(cm_clf2)\n",
    "\n",
    "print(\"Confusion Matrix for clf:\\n\", cm_clf)\n",
    "print(f\"Sensitivity (clf): {sensitivity_clf:.2f}\")\n",
    "print(f\"Specificity (clf): {specificity_clf:.2f}\")\n",
    "print(f\"Accuracy (clf): {accuracy_clf:.2f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix for clf2:\\n\", cm_clf2)\n",
    "print(f\"Sensitivity (clf2): {sensitivity_clf2:.2f}\")\n",
    "print(f\"Specificity (clf2): {specificity_clf2:.2f}\")\n",
    "print(f\"Accuracy (clf2): {accuracy_clf2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365bc65",
   "metadata": {},
   "source": [
    "Confusion Matrix: Shows counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "Metrics:\n",
    "Sensitivity: Measures how well the model identifies positive cases.\n",
    "Specificity: Measures how well the model identifies negative cases.\n",
    "Accuracy: Measures the overall correctness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790dd7b",
   "metadata": {},
   "source": [
    "7: Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for clf and clf2) are better\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(ab_reduced_noNaN_train[['List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(\n",
    "                         ab_reduced_noNaN_train[['NumPages','Thick','List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df4b77",
   "metadata": {},
   "source": [
    "The differences between the two confusion matrices are because the first model (clf) only uses the List Price feature, while the second one adds extra features like NumPages and Thick. By adding more features, the model gets more information, which can improve or hurt predictions depending on how relevant those features are. The confusion matrices for clf and clf2 are better because they test the models on test data instead of training data. Using training data can make the model look better than it actually is because it already saw that data, while testing on unseen data shows how well the model performs in real-world situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b06836",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbotlink: https://chatgpt.com/share/67400f36-ea20-8012-ba80-b9c0415ba9b5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b60394",
   "metadata": {},
   "source": [
    "Summarise: \n",
    "\n",
    "Markov Chains and Text Generation: You are working on generating sequences of words using Markov Chains for your story with multiple characters and dialogues, exploring ways to adapt the model to handle character-specific interactions. You also considered using n-grams for text generation.\n",
    "\n",
    "Data Visualization: You explored various visualization techniques, including KDE plots, box plots, and histograms, and decided that KDE plots are ideal for visualizing trends, while box plots are better for comparing different datasets or identifying outliers. You also discussed using these techniques in your dataset related to bee colony loss.\n",
    "\n",
    "Statistical Concepts: You examined the relationship between mean, median, and skewness in distributions and implemented statistical modeling using the 'pokemon_data.csv' dataset, including linear regression and hypothesis testing with R-squared and p-values. You also looked at bootstrapping techniques for vaccine data analysis and performed simulations.\n",
    "\n",
    "Python Programming: You learned and discussed various Python techniques, including common string and list methods, avoiding advanced techniques like in, break, and .values(). You also worked on functions like count_odds_from_file and convert_data.\n",
    "\n",
    "Decision Tree Classification: You are using scikit-learn’s DecisionTreeClassifier to predict whether a book is hardcover or paperback based on the List Price variable. You also trained two classifiers (clf and clf2) with different feature sets and evaluated their performance using confusion matrices. You computed metrics like accuracy, sensitivity, and specificity for both models.\n",
    "\n",
    "Confusion Matrix and Model Comparison: You asked about comparing confusion matrices for models trained on different feature sets, where the model with additional features (NumPages, Thick, and List Price) potentially improves performance. The importance of fine-tuning models like clf and clf2 for optimal performance was emphasized.\n",
    "\n",
    "Rendering Visualizations: You preferred visualizations to be rendered as PNG using fig.show(renderer=\"png\") for clarity and simplicity in presenting figures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
